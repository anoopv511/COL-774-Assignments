\documentclass[12pt]{article}
\usepackage[margin=0.5in,top=0.5in,bottom=0.5in]{geometry}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{scrextend}
\usepackage{graphicx}
\usepackage{wrapfig}
\graphicspath{{images/}}

\title{\textbf{\underline{Assignment 3 Report}}}
\author{Anoop (2015CS10265)}

\begin{document}
\pagenumbering{gobble}
\maketitle

\section*{\underline{Decision Trees}}
\subsection*{Decision Tree Implementation}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Number of Nodes \qquad \= = 7972 \\
Train Accuracy \> = 0.88933 \\
Validation Accuracy \> = 0.79933 \\
Test Accuracy  \> = 0.80671 \\
\end{tabbing}
\end{addmargin}
\includegraphics[width=0.5\textwidth]{dtree1.png}
\includegraphics[width=0.5\textwidth]{dtree2.png}
\vspace{1mm}
\begin{addmargin}[0.3in]{0in}
From both BFS and DFS ways of growing the decision tree, we can observe that the training accuracy increases with number of nodes. Also from the validation and test accuracy curves we can say that the decision tree is overfitting the data. The decision tree is not able to fit the data completely because of the pre-processing that has been done on the data (There are examples where for the same features, the labels are different).
\end{addmargin}
\subsection*{Post-Pruning}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Number of Nodes \qquad \= = 2322 \\
Train Accuracy \> = 0.84893 \\
Validation Accuracy \> = 0.83467 \\
Test Accuracy  \> = 0.82557 \\
\end{tabbing}
\end{addmargin}
\includegraphics[width=0.5\textwidth]{dtree3.png}
\vspace{1mm}
\begin{addmargin}[0.3in]{0in}
From the graph, we can observe that post-pruning is reducing the overfitting. Also, as the validation accuracy is increasing, the test accuracy is also increasing.
\end{addmargin}
\subsection*{Decision Tree Implementation (Handling numerical features)}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Number of Nodes \qquad \= = 16495 \\
Train Accuracy \> = 0.99989 \\
Validation Accuracy \> = 0.78800 \\
Test Accuracy  \> = 0.77914 \\
\end{tabbing}
\end{addmargin}
\includegraphics[width=0.5\textwidth]{dtree4.png}
\includegraphics[width=0.5\textwidth]{dtree5.png}
\vspace{1mm}
\begin{addmargin}[0.3in]{0in}
In this modified implementation of decision trees, we are better able to handle numerical attributes. This can be seen in the training accuracy which is close to 100\%. The decision tree has almost completely fit the training data. This overfitting has brought down the validation and test accuracies.
\end{addmargin}
\vspace{2mm}
\begin{addmargin}[0.3in]{0in}
\begin{table}[h!]
\begin{center}
\caption{\underline{Number of splits per numerical feature}}
\vspace{1mm}
\label{tab:table1}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Age} & \textbf{Fnlwgt} & \textbf{Education Number} & \textbf{Capital Gain} & \textbf{Capital Loss} & \textbf{Hour per Week} \\
\hline
3446 & 2194 & 0 & 18 & 6 & 536 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{addmargin}
\includegraphics[width=0.5\textwidth]{dtree6.png}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Number of Nodes \qquad \= = 3650 \\
Train Accuracy \> = 0.87230 \\
Validation Accuracy \> = 0.84167 \\
Test Accuracy \> = 0.81429 \\
\end{tabbing}
\vspace{-3mm}
\end{addmargin}
\begin{addmargin}[0.3in]{0in}
Post-Pruning the fully grown decision tree (modified) gives even better validation accuracy but lesser test accuracy.
\end{addmargin}
\subsection*{Decision Tree Implementation - Scikit-Learn}
\vspace{-7mm}
\begin{table}[h!]
\begin{center}
\caption{\underline{Best Parameters (from 2160 models)}}
\vspace{1mm}
\label{tab:table2}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{criterion} & \textbf{max\_depth} & \textbf{min\_samples\_split} & \textbf{min\_samples\_leaf} & \textbf{max\_features} \\
\hline
1 & gini & 12 & 0.005 & 0.001 & None \\
2 & entropy & 10 & 0.005 & 0.001 & None \\
\hline
\end{tabular}
\end{center}
\end{table}
\vspace{-13mm}
\begin{table}[h!]
\begin{center}
\caption{\underline{Accuracies}}
\vspace{1mm}
\label{tab:table3}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Train Accuracy} & \textbf{Validation Accuracy} & \textbf{Test Accuracy} \\
\hline
1 & 0.83507 & 0.82633 & 0.82914 \\
2 & 0.83574 & 0.82633 & 0.83000 \\
\hline
\end{tabular}
\end{center}
\end{table}
\vspace{-5mm}
\begin{addmargin}[0.3in]{0in}
\textbf{max\_depth} restricts the depth of the decision tree, \textbf{min\_samples\_split} restricts the splitting of small nodes and \textbf{min\_samples\_leaf} enforces a minimum size requirement for a node to be a leaf. All these hyperparameters help in reducing overfitting. As compared to greedy post-pruning, these hyperparameters need to be tuned manually. In this case, searching for the hyperparameters helps rather than greedy post-pruning marginally (over test accuracies).
\end{addmargin}
\subsection*{Random Forest Implementation - Scikit-Learn}
\vspace{-7mm}
\begin{table}[h!]
\begin{center}
\caption{\underline{Best Parameters (from 17280 models)}}
\vspace{1mm}
\label{tab:table2}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{criterion} & \textbf{n\_estimators} & \textbf{max} & \textbf{min} & \textbf{min} & \textbf{max} & \textbf{bootstrap} \\
&  &  & \textbf{\_depth} & \textbf{\_samples} & \textbf{\_samples} & \textbf{\_features} & \\
&  &  &  & \textbf{\_split} & \textbf{\_leaf} &  & \\
\hline
1 & gini & 2 & 8 & 0.01 & 1 & 10 & False\\
2 & entropy & 2 & 12 & 0.01 & 0.001 & 10 & True\\
\hline
\end{tabular}
\end{center}
\end{table}
\begin{table}[h!]
\begin{center}
\caption{\underline{Accuracies}}
\vspace{1mm}
\label{tab:table3}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Train Accuracy} & \textbf{Validation Accuracy} & \textbf{Test Accuracy} \\
\hline
1 & 0.83263 & 0.82867 & 0.82586 \\
2 & 0.83119 & 0.82933 & 0.82771 \\
\hline
\end{tabular}
\end{center}
\end{table}
\vspace{-5mm}
\begin{addmargin}[0.3in]{0in}
\textbf{n\_estimators} specifies the number of decision trees in the random forest , \textbf{max\_features} restricts the number of features searched over while choosing the split and \textbf{bootstrap} specifies if bootstrap samples are used or not. As compared to greedy post-pruning, these hyperparameters need to be tuned manually. In this case also, searching for the hyperparameters helps rather than greedy post-pruning marginally (over test accuracies).
\end{addmargin}

\newpage
\section*{\underline{Neural Networks}}
\subsection*{Logistic Regression - Scikit-Learn}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Train Accuracy \= = 0.46053 \\
Test Accuracy \> = 0.35000
\end{tabbing}
\end{addmargin}
\includegraphics[width=0.4\textwidth]{log1.png}
\includegraphics[width=0.4\textwidth]{log2.png}
\subsection*{Neural Network - 5 Hidden Units}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Training Time \quad \= = 2.74530s \\
Train Accuracy \> = 0.89737 \\
Test Accuracy \> = 0.85000
\end{tabbing}
\end{addmargin}
\includegraphics[width=0.4\textwidth]{nn1.png}
\includegraphics[width=0.4\textwidth]{nn2.png}
\subsection*{Neural Networks - Multiple Hidden Units}
\begin{table}[h!]
\begin{center}
\caption{\underline{Hidden Units - Accuracies}}
\vspace{1mm}
\label{tab:table6}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Hidden Units} & \textbf{Training Time (s)} & \textbf{Train Accuracy} & \textbf{Test Accuracy} \\
\hline
1 & 1.51454 & 0.64211 & 0.56667 \\
2 & 1.88829 & 0.60789 & 0.58333 \\
3 & 2.07101 & 0.90000 & 0.85833 \\
10 & 2.93981 & 0.91053 & 0.82500 \\
20 & 4.50854 & 0.91316 & 0.82500 \\
40 & 10.57310 & 0.91053 & 0.82500 \\
\hline
\end{tabular}
\end{center}
\end{table}
\begin{addmargin}[0in]{0in}
\includegraphics[width=0.5\textwidth]{nn3.png}
\includegraphics[width=0.5\textwidth]{nn4.png} \\ \\
\includegraphics[width=0.5\textwidth]{nn5.png}
\includegraphics[width=0.5\textwidth]{nn6.png} \\ \\
\includegraphics[width=0.5\textwidth]{nn7.png}
\includegraphics[width=0.5\textwidth]{nn8.png} \\ \\
\end{addmargin}
\newpage
\subsection*{Neural Networks - 2 Hidden Layers}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Training Time \quad \= = 3.62151s \\
Train Accuracy \> = 0.90000 \\
Test Accuracy \> = 0.85833
\end{tabbing}
\end{addmargin}
\includegraphics[width=0.5\textwidth]{nn9.png}
\includegraphics[width=0.5\textwidth]{nn10.png}
\vspace{5mm}
\begin{addmargin}[0.3in]{0in}
The decision boundary tries to include more examples (overfitting) as the number of hidden units increase. The ideal number of hidden units is 3 units - Gives a good train and test accuracies and trains quickly. With two hidden layers there is not much change in accuracy but it still takes more time to train.
\end{addmargin}
\subsection*{Working with MNIST}
\subsubsection*{LIBSVM}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Training Time \quad \= = 6.62021s \\
Train Accuracy \= = 1.00 \\
Test Accuracy \= = 0.98472
\end{tabbing}
\end{addmargin}
\subsubsection*{Single Perceptron}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Training Time \quad \= = 10.87574s \\
Train Accuracy \= = 0.97470 \\
Test Accuracy \= = 0.97556
\end{tabbing}
\end{addmargin}
\subsubsection*{Neural Network - Dynamic Learning Rate}
\begin{addmargin}[0.3in]{0in}
\begin{tabbing}
Training Time \quad \= = 159.88187s \\
Train Accuracy \= = 0.99880 \\
Test Accuracy \= = 0.99306
\end{tabbing}
\end{addmargin}
\vspace{5mm}
\begin{addmargin}[0.3in]{0in}
LIBSVM does better than a single perceptron in faster time. But the single perceptron can be trained for longer time to get better accuracies. With dynamic learning rate, we get the best test accuracy though it takes longer to train.
\end{addmargin}

\end{document}